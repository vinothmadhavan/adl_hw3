from typing import overload

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceTB/SmolLM2-360M-Instruct"

device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"


class BaseLLM:
    def __init__(self, checkpoint=checkpoint):
        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
        self.model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)
        self.device = device

    def format_prompt(self, question: str) -> str:
        """
        Take a question and convert it into an input to SmolLM2. The LLM will likely answer much
        better if you provide a chat template. self.tokenizer.apply_chat_template can help here
        """
        
        # # question="Answer the following question:"+question
        # question="""
        # Answer the following question. Response should be float number.
        # Can you change 2 hour to its equivalent in min?
        # For eg: How many gram are there per 6 kg?
        # answer should be <answer>6000.0</answer>
        # """
        return question
        # return question

    def parse_answer(self, answer: str) -> float:
        """
        Parse the <answer></answer> tag and return a float.
        This function is somewhat robust to output errors (e.g. missing </answer> tags).
        """
        try:
            return float(answer.split("<answer>")[1].split("</answer>")[0])
        except (IndexError, ValueError):
            return float("nan")

    def generate(self, prompt: str) -> str:
        """
        (Optional) Implement this method first and then implement batched_generate below.
        It is much easier to implement generation without batching.

        The overall flow is the same:
        - tokenize the prompt with self.tokenizer
        - call self.model.generate
        - decode the outputs with self.tokenizer.decode

        """
        return self.batched_generate([prompt])[0]

    @overload
    def batched_generate(
        self, prompts: list[str], num_return_sequences: None = None, temperature: float = 0
    ) -> list[str]:
        """
        Batched version of `generate` method.
        This version returns a single generation for each prompt.
        """

    @overload
    def batched_generate(
        self, prompts: list[str], num_return_sequences: int, temperature: float = 0
    ) -> list[list[str]]:
        """
        Batched version of `generate` method.
        This version returns a list of generation for each prompt.
        """

    # def batched_generate(
    #     self, prompts: list[str], num_return_sequences: int | None = None, temperature: float = 0
    # ) -> list[str] | list[list[str]]:
    #     """
    #     Batched version of `generate` method.

    #     You will likely get an up to 10x speedup using batched decoding.

    #     To implement batch decoding you will need to:
    #     - tokenize the prompts self.tokenizer with padding=True and return_tensors="pt"
    #     - call self.model.generate
    #     - decode the outputs with self.tokenizer.batch_decode

    #     Tip: You need to set self.tokenizer.padding_side = "left" to get the correct padding behavior for generation.
    #          Left padding makes sure all sequences are aligned to the right (i.e. where tokens are generated).
    #     Tip: self.model.generate takes a lot of parameters. Here are some relevant ones:
    #         - max_new_tokens: The maximum number of tokens to generate. Set this to a reasonable value
    #                           (50 should suffice).
    #         - do_sample and temperature: For any temperature > 0, set do_sample=True.
    #                                      do_sample=False will use greedy decoding.
    #         - num_return_sequences: The number of sequences to return. Note that this will generate a flat
    #                                 list of len(prompts) * num_return_sequences entries.
    #         - eos_token_id: The end of sequence token id. This is used to stop generation. Set this
    #                         to self.tokenizer.eos_token_id.
    #     Pro Tip: Only batch_decode generated tokens by masking out the inputs with
    #              outputs[:, len(inputs["input_ids"][0]) :]
    #     """
    #     from tqdm import tqdm  # Importing tqdm for progress bar

    #     # Preventing OOM
    #     # Depending on your GPU batched generation will use a lot of memory.
    #     # If you run out of memory, try to reduce the micro_batch_size.
    #     micro_batch_size = 32
    #     if len(prompts) > micro_batch_size:
    #         return [
    #             r
    #             for idx in tqdm(
    #                 range(0, len(prompts), micro_batch_size), desc=f"LLM Running on Micro Batches {micro_batch_size}"
    #             )
    #             for r in self.batched_generate(prompts[idx : idx + micro_batch_size], num_return_sequences, temperature)
    #         ]

    #     # Set left padding for generation
    #     self.tokenizer.padding_side = "left"

    #     # Tokenize prompts
    #     inputs = self.tokenizer(
    #         prompts,
    #         padding=True,
    #         return_tensors="pt",
    #     )
    #     input_ids = inputs["input_ids"].to(self.device)
    #     attention_mask = inputs["attention_mask"].to(self.device)

    #     # Generation parameters
    #     max_new_tokens = 50
    #     do_sample = temperature > 0
    #     num_return_sequences = num_return_sequences if num_return_sequences is not None else 1

    #     with torch.no_grad():
    #         outputs = self.model.generate(
    #             input_ids=input_ids,
    #             attention_mask=attention_mask,
    #             max_new_tokens=max_new_tokens,
    #             do_sample=do_sample,
    #             temperature=temperature if do_sample else None,
    #             num_return_sequences=num_return_sequences,
    #             eos_token_id=self.tokenizer.eos_token_id,
    #             pad_token_id=self.tokenizer.pad_token_id,
    #         )

    #     # outputs shape: (batch_size * num_return_sequences, seq_len)
    #     # Only decode the generated part (not the prompt)
    #     # For left padding, the prompt length is the same for all in the batch
    #     prompt_lengths = [x.sum().item() for x in attention_mask]
    #     # If all prompts are the same length, we can use a single value
    #     min_prompt_len = min(prompt_lengths)
    #     # For each output, slice off the prompt tokens
    #     # outputs is (batch_size * num_return_sequences, seq_len)
    #     # If num_return_sequences > 1, outputs are grouped per prompt

    #     # If num_return_sequences > 1, outputs are grouped per prompt
    #     # So, for each prompt, there are num_return_sequences outputs in a row

    #     # For each output, find the corresponding prompt length
    #     total_outputs = outputs.shape[0]
    #     batch_size = len(prompts)
    #     decoded = []
    #     for i in range(total_outputs):
    #         # For each output, find which prompt it corresponds to
    #         prompt_idx = i % batch_size
    #         prompt_len = prompt_lengths[prompt_idx]
    #         # Only decode the generated part
    #         decoded.append(
    #             self.tokenizer.decode(
    #                 outputs[i, prompt_len:],
    #                 skip_special_tokens=True,
    #             ).strip()
    #         )

    #     # If num_return_sequences == 1, return a flat list
    #     if num_return_sequences == 1:
    #         return decoded
    #     else:
    #         # Group into list of lists
    #         grouped = []
    #         for i in range(batch_size):
    #             grouped.append(decoded[i::batch_size])
    #         return grouped


    def batched_generate(
        self, prompts: list[str], num_return_sequences: int | None = None, temperature: float = 0
    ) -> list[str] | list[list[str]]:
        from tqdm import tqdm  # Importing tqdm for progress bar

        micro_batch_size = 32
        if len(prompts) > micro_batch_size:
            return [
                r
                for idx in tqdm(
                    range(0, len(prompts), micro_batch_size), desc=f"LLM Running on Micro Batches {micro_batch_size}"
                )
                for r in self.batched_generate(prompts[idx : idx + micro_batch_size], num_return_sequences, temperature)
            ]

        self.tokenizer.padding_side = "left"
        # print("inside batched_generate")
        # print("prompt",prompts)
        inputs = self.tokenizer(
            prompts,
            padding=True,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        max_new_tokens = 500
        do_sample = temperature > 0
        num_return_sequences = num_return_sequences if num_return_sequences is not None else 1

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature if do_sample else None,
                num_return_sequences=num_return_sequences,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        prompt_lengths = [x.sum().item() for x in attention_mask]
        batch_size = len(prompts)
        
        # print("outputs",outputs)

        decoded = []
        for i in range(outputs.shape[0]):
            prompt_idx = i // num_return_sequences
            prompt_len = prompt_lengths[prompt_idx]
            decoded.append(
                self.tokenizer.decode(
                    outputs[i, prompt_len:],
                    skip_special_tokens=True,
                ).strip()
            )
            # print("outputs.shape[0]",outputs.shape[0])
            # print("num_return_sequences",num_return_sequences)
            # print("prompt_len",prompt_len)
            # print("prompt_lengths",prompt_lengths)
            # print("decoded",decoded)
            # print("decoded1",
            #     self.tokenizer.decode(
            #         outputs[i],
            #         skip_special_tokens=False,
            #     ).strip()
            # )

        if num_return_sequences == 1:
            return decoded
        else:
            grouped = []
            for i in range(batch_size):
                start = i * num_return_sequences
                end = (i + 1) * num_return_sequences
                grouped.append(decoded[start:end])
            return grouped
        
        
    def answer(self, *questions) -> list[float]:
        """
        Answer questions given as individual string arguments.
        """
        # Convert each question
        prompts = [self.format_prompt(q) for q in questions]
        generations = self.batched_generate(prompts)
        return [self.parse_answer(g) for g in generations]


def test_model():
    # The following code simply tests of the BaseLLM is able to complete text.
    # It should produce garbage answers, but it should not crash.
    # In my case it talks about cats eating cats, and dogs being happy.
    testset = ["The cat went up", "The dog went down"]
    model = BaseLLM()
    for t in testset:
        print("testing generate function")
        print("input", t)
        answer = model.generate(t)
        print("output", answer)
    answers = model.batched_generate(testset)
    print(answers)


if __name__ == "__main__":
    from fire import Fire

    Fire({"test": test_model})

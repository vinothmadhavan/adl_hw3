def tokenize(tokenizer, question: str, answer: str):
    # Compose the full text as before
    full_text = f"{question} {answer}{tokenizer.eos_token}"

    tokenizer.padding_side = "right"
    tokenizer.pad_token = tokenizer.eos_token
    full = tokenizer(full_text, padding="max_length", truncation=True, max_length=128)

    input_ids = full["input_ids"]

    # Tokenize the answer (with no leading space)
    answer_ids = tokenizer(answer, add_special_tokens=False)["input_ids"]

    # Find the start index of answer_ids in input_ids
    def find_sublist(lst, sublst):
        for i in range(len(lst) - len(sublst) + 1):
            if lst[i:i+len(sublst)] == sublst:
                return i
        return -1

    answer_start = find_sublist(input_ids, answer_ids)
    if answer_start == -1:
        # fallback: try with a leading space
        answer_ids = tokenizer(" " + answer, add_special_tokens=False)["input_ids"]
        answer_start = find_sublist(input_ids, answer_ids)
        if answer_start == -1:
            print("Warning: answer tokens not found in input_ids!")
            labels = [-100] * len(input_ids)
        else:
            labels = [-100] * answer_start + input_ids[answer_start:answer_start+len(answer_ids)] + [-100] * (len(input_ids) - answer_start - len(answer_ids))
    else:
        labels = [-100] * answer_start + input_ids[answer_start:answer_start+len(answer_ids)] + [-100] * (len(input_ids) - answer_start - len(answer_ids))

    # Mask out padding
    for i in range(len(labels)):
        if full["attention_mask"][i] == 0:
            labels[i] = -100

    full["labels"] = labels
    return full
